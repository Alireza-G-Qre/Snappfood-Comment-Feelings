{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scrapy in /home/alireza/.local/lib/python3.8/site-packages (2.6.1)\n",
      "Requirement already satisfied: Twisted>=17.9.0 in /home/alireza/.local/lib/python3.8/site-packages (from scrapy) (22.4.0)\n",
      "Requirement already satisfied: w3lib>=1.17.0 in /home/alireza/.local/lib/python3.8/site-packages (from scrapy) (1.22.0)\n",
      "Requirement already satisfied: itemloaders>=1.0.1 in /home/alireza/.local/lib/python3.8/site-packages (from scrapy) (1.0.4)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from scrapy) (45.2.0)\n",
      "Requirement already satisfied: PyDispatcher>=2.0.5; platform_python_implementation == \"CPython\" in /home/alireza/.local/lib/python3.8/site-packages (from scrapy) (2.0.5)\n",
      "Requirement already satisfied: cssselect>=0.9.1 in /home/alireza/.local/lib/python3.8/site-packages (from scrapy) (1.1.0)\n",
      "Requirement already satisfied: cryptography>=2.0 in /usr/lib/python3/dist-packages (from scrapy) (2.8)\n",
      "Requirement already satisfied: service-identity>=16.0.0 in /home/alireza/.local/lib/python3.8/site-packages (from scrapy) (21.1.0)\n",
      "Requirement already satisfied: pyOpenSSL>=16.2.0 in /home/alireza/.local/lib/python3.8/site-packages (from scrapy) (22.0.0)\n",
      "Requirement already satisfied: itemadapter>=0.1.0 in /home/alireza/.local/lib/python3.8/site-packages (from scrapy) (0.5.0)\n",
      "Requirement already satisfied: protego>=0.1.15 in /home/alireza/.local/lib/python3.8/site-packages (from scrapy) (0.2.1)\n",
      "Requirement already satisfied: lxml>=3.5.0; platform_python_implementation == \"CPython\" in /home/alireza/.local/lib/python3.8/site-packages (from scrapy) (4.8.0)\n",
      "Requirement already satisfied: parsel>=1.5.0 in /home/alireza/.local/lib/python3.8/site-packages (from scrapy) (1.6.0)\n",
      "Requirement already satisfied: queuelib>=1.4.2 in /home/alireza/.local/lib/python3.8/site-packages (from scrapy) (1.6.2)\n",
      "Requirement already satisfied: tldextract in /home/alireza/.local/lib/python3.8/site-packages (from scrapy) (3.2.1)\n",
      "Requirement already satisfied: zope.interface>=4.1.3 in /home/alireza/.local/lib/python3.8/site-packages (from scrapy) (5.4.0)\n",
      "Requirement already satisfied: Automat>=0.8.0 in /home/alireza/.local/lib/python3.8/site-packages (from Twisted>=17.9.0->scrapy) (20.2.0)\n",
      "Requirement already satisfied: incremental>=21.3.0 in /home/alireza/.local/lib/python3.8/site-packages (from Twisted>=17.9.0->scrapy) (21.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.5 in /home/alireza/.local/lib/python3.8/site-packages (from Twisted>=17.9.0->scrapy) (4.2.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /usr/lib/python3/dist-packages (from Twisted>=17.9.0->scrapy) (19.3.0)\n",
      "Requirement already satisfied: hyperlink>=17.1.1 in /home/alireza/.local/lib/python3.8/site-packages (from Twisted>=17.9.0->scrapy) (21.0.0)\n",
      "Requirement already satisfied: constantly>=15.1 in /home/alireza/.local/lib/python3.8/site-packages (from Twisted>=17.9.0->scrapy) (15.1.0)\n",
      "Requirement already satisfied: six>=1.4.1 in /usr/lib/python3/dist-packages (from w3lib>=1.17.0->scrapy) (1.14.0)\n",
      "Requirement already satisfied: jmespath>=0.9.5 in /home/alireza/.local/lib/python3.8/site-packages (from itemloaders>=1.0.1->scrapy) (1.0.0)\n",
      "Requirement already satisfied: pyasn1-modules in /home/alireza/.local/lib/python3.8/site-packages (from service-identity>=16.0.0->scrapy) (0.2.8)\n",
      "Requirement already satisfied: pyasn1 in /home/alireza/.local/lib/python3.8/site-packages (from service-identity>=16.0.0->scrapy) (0.4.8)\n",
      "Requirement already satisfied: idna in /usr/lib/python3/dist-packages (from tldextract->scrapy) (2.8)\n",
      "Requirement already satisfied: requests>=2.1.0 in /usr/lib/python3/dist-packages (from tldextract->scrapy) (2.22.0)\n",
      "Requirement already satisfied: requests-file>=1.4 in /home/alireza/.local/lib/python3.8/site-packages (from tldextract->scrapy) (1.5.1)\n",
      "Requirement already satisfied: filelock>=3.0.8 in /usr/lib/python3/dist-packages (from tldextract->scrapy) (3.0.12)\n",
      "Requirement already satisfied: bs4 in /home/alireza/.local/lib/python3.8/site-packages (0.0.1)\n",
      "Requirement already satisfied: beautifulsoup4 in /home/alireza/.local/lib/python3.8/site-packages (from bs4) (4.11.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/alireza/.local/lib/python3.8/site-packages (from beautifulsoup4->bs4) (2.3.2.post1)\n"
     ]
    }
   ],
   "source": [
    "!pip install scrapy\n",
    "!pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_selectors = {\n",
    "    'imdb.com': [\n",
    "        ('title', ['div.sc-94726ce4-1.iNShGo > h1',]),\n",
    "        ('score', ['div.sc-db8c1937-0.eGmDjE.sc-94726ce4-4.dyFVGl div.sc-7ab21ed2-2.kYEdvH > span.sc-7ab21ed2-1.jGRxWM',]),\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scrapy import Item, Field\n",
    "\n",
    "class Imdb(Item):\n",
    "    title = Field()\n",
    "    score = Field()\n",
    "    \n",
    "customItem = Imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "allowed_domains = ['digikala.com']\n",
    "name = 'digikala'\n",
    "\n",
    "allows = [r'https://www.digikala.com/product/dkp-',]\n",
    "start_urls = ['https://www.digikala.com/product/dkp-7527323/%D8%AA%DB%8C-%D8%B4%D8%B1%D8%AA-%D8%B2%D9%86%D8%A7%D9%86%D9%87-%D8%A8%D8%A7%DB%8C%D9%86%D8%AA-%D9%85%D8%AF%D9%84-2261448-59/',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC\n",
    "import scrapy\n",
    "import time\n",
    "\n",
    "from scrapy.spiders import CrawlSpider, Rule\n",
    "from scrapy.linkextractors import LinkExtractor\n",
    "\n",
    "from bs4 import BeautifulSoup as Soup\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "\n",
    "IGNORED_EXTENSIONS = [\n",
    "    'mng', 'pct', 'bmp', 'gif', 'jpg', 'jpeg', 'png', 'pst', 'psp', 'tif',\n",
    "    'tiff', 'ai', 'drw', 'dxf', 'eps', 'ps', 'svg',\n",
    "    'mp3', 'wma', 'ogg', 'wav', 'ra', 'aac', 'mid', 'au', 'aiff',\n",
    "    '3gp', 'asf', 'asx', 'avi', 'mov', 'mp4', 'mpg', 'qt', 'rm', 'swf', 'wmv',\n",
    "    'm4a', 'css', 'pdf', 'doc', 'exe', 'bin', 'rss', 'zip', 'rar',\n",
    "]\n",
    "\n",
    "IGNORED_EXTENSIONS = set(IGNORED_EXTENSIONS)    \n",
    "    \n",
    "    \n",
    "class DomainSpider(CrawlSpider, ABC):\n",
    "    \n",
    "    minwords = 1\n",
    "    name = 'do_crawler'\n",
    "    start_urls = start_urls\n",
    "    allowed_domains = allowed_domains\n",
    "    \n",
    "    counter = 0\n",
    "    \n",
    "    rules = (\n",
    "        Rule(LinkExtractor(\n",
    "            allow=allows, deny_extensions=IGNORED_EXTENSIONS), callback='parse_item', follow=True),\n",
    "    )\n",
    "\n",
    "    def parse_item(self, response):\n",
    "        self.logger.info('parsed: %s', response.url)\n",
    "        \n",
    "        try:\n",
    "            \n",
    "            domain = urlparse(response.url).netloc\n",
    "            domain = domain[4:] if domain.startswith('www.') else domain\n",
    "\n",
    "            sp = Soup(response.text, 'lxml')\n",
    "\n",
    "            if domain not in domain_selectors:\n",
    "                return\n",
    "\n",
    "            found = False\n",
    "            item = customItem()\n",
    "\n",
    "            for title, selectors in domain_selectors[domain]:\n",
    "\n",
    "                parsed = sum([sp.select(s) for s in selectors], [])\n",
    "                parts = [x.get_text() for x in parsed]\n",
    "                text = ' '.join(parts)\n",
    "\n",
    "                if len(text.split()) >= self.minwords: \n",
    "                    item[title] = text\n",
    "                    found = True\n",
    "\n",
    "            if found: \n",
    "                \n",
    "                if self.counter < 10: print(F'10F results: {item}')\n",
    "                self.counter = self.counter + 1\n",
    "                return item\n",
    "        \n",
    "        except:\n",
    "            self.logger.warning('failed(status:%s) %s' % (response.status, response.url))\n",
    "\n",
    "            \n",
    "from scrapy.utils.project import get_project_settings\n",
    "\n",
    "settings = get_project_settings()\n",
    "\n",
    "settings.update({\n",
    "    'FEEDS': {F\"{name}-{int(time.time())}.csv\": {\"format\": \"csv\"},},\n",
    "    'LOG_LEVEL': 'WARNING'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10F results: {'score': '8.8', 'title': 'Inception'}\n",
      "10F results: {'score': '8.2', 'title': 'Batman Begins'}\n",
      "10F results: {'score': '8.4', 'title': 'The Dark Knight Rises'}\n",
      "10F results: {'score': '8.8', 'title': 'Inception'}\n",
      "10F results: {'score': '8.4', 'title': 'The Dark Knight Rises'}\n",
      "10F results: {'score': '9.3', 'title': 'The Shawshank Redemption'}\n",
      "10F results: {'score': '8.2', 'title': 'Batman Begins'}\n",
      "10F results: {'score': '7.5', 'title': 'Jackie Brown'}\n",
      "10F results: {'score': '7.5', 'title': 'Beetle Juice'}\n",
      "10F results: {'score': '9.3', 'title': 'The Shawshank Redemption'}\n"
     ]
    }
   ],
   "source": [
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "process = CrawlerProcess(settings=settings)\n",
    "process.crawl(DomainSpider)\n",
    "process.start() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ir-venv",
   "language": "python",
   "name": "ir-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
