{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install pandas hazm langdetect CPVI tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading DataFrame & Keep Persian Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals\n",
    "from langdetect import detect\n",
    "\n",
    "import pandas as pd\n",
    "import random as ra\n",
    "\n",
    "df = pd.read_csv('./comment-feeling-1650617369.csv')\n",
    "\n",
    "def is_persian(text):\n",
    "    try: return detect(text) == 'fa' \n",
    "    except: return False\n",
    "    \n",
    "df = df[df.commentText.apply(lambda x: is_persian(x))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scenario 1: Normalize The Text Of The Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hazm import Normalizer\n",
    "\n",
    "normalizer = Normalizer()\n",
    "df['normalized'] = df.commentText.apply(lambda x: normalizer.normalize(x).strip())\n",
    "\n",
    "df = df[df.normalized.str.count(' ') > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n'.join(df.normalized.sample(10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scenario 2: Normalize & Delete Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hazm import stopwords_list, word_tokenize\n",
    "\n",
    "stopwords = set(stopwords_list())\n",
    "df['no_stopwords'] = df.commentText.apply(\n",
    "    lambda x: ' '.join((y for y in word_tokenize(x) if y not in stopwords)))\n",
    "\n",
    "df = df[df.no_stopwords.str.count(' ') > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n'.join(df.no_stopwords.sample(10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scenario 3: Stemming & lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hazm import Lemmatizer, Stemmer, word_tokenize\n",
    "\n",
    "stemmer = Stemmer()\n",
    "lemmatizer = Lemmatizer()\n",
    "\n",
    "def context_exchanger(word):\n",
    "    \n",
    "    lemmatized, stemmed = lemmatizer.lemmatize(word), stemmer.stem(word) \n",
    "    \n",
    "    return (('ن' if word.startswith('ن') and not lemmatized.split('#')[1].startswith('ن') else '') \n",
    "            + lemmatized.split('#')[1]) if '#' in lemmatized else stemmed\n",
    "\n",
    "df['lemz_or_stem'] = df.commentText.apply(\n",
    "    lambda x: ' '.join((context_exchanger(y) for y in word_tokenize(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n'.join(df.lemz_or_stem.sample(10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scenario 4: Using POSTagger & Chunker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hazm import POSTagger, word_tokenize\n",
    "from pprint import pprint\n",
    "\n",
    "tagger = POSTagger(model='../resources/postagger.model')\n",
    "\n",
    "for text in df.commentText.sample(1):\n",
    "    pprint(text)\n",
    "    pprint(tagger.tag(word_tokenize(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hazm import POSTagger, Chunker, tree2brackets, word_tokenize\n",
    "\n",
    "tagger = POSTagger(model='../resources/postagger.model')\n",
    "chunker = Chunker(model='../resources/chunker.model')\n",
    "\n",
    "for text in df.commentText.sample(1):\n",
    "    pprint(text)\n",
    "    tagged = tagger.tag(word_tokenize(text))\n",
    "    pprint(tree2brackets(chunker.parse(tagged)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scenario 5: Changing Shape Of Verb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "from CPVI import CPVI\n",
    "\n",
    "\n",
    "flatten_list = lambda irregular_list:[element for item in irregular_list for element in flatten_list(item)] \\\n",
    "                                        if type(irregular_list) is list else [irregular_list]\n",
    "\n",
    "def get_allshapes(verb):\n",
    "    \n",
    "    profile = CPVI().profiling(verb)\n",
    "    \n",
    "    verbs = []\n",
    "    \n",
    "    verbs.extend(profile['paradigm']['informal']['Persian']['affirmative']['present'].values())\n",
    "    verbs.extend(profile['paradigm']['informal']['Persian']['affirmative']['past'].values())\n",
    "    verbs.extend(profile['paradigm']['informal']['Persian']['affirmative']['future'].values())\n",
    "    verbs.extend(profile['paradigm']['informal']['Persian']['negative']['present'].values())\n",
    "    verbs.extend(profile['paradigm']['informal']['Persian']['negative']['past'].values())\n",
    "    verbs.extend(profile['paradigm']['informal']['Persian']['negative']['future'].values())\n",
    "    verbs.extend(profile['paradigm']['formal']['Persian']['affirmative']['present'].values())\n",
    "    verbs.extend(profile['paradigm']['formal']['Persian']['affirmative']['past'].values())\n",
    "    verbs.extend(profile['paradigm']['formal']['Persian']['affirmative']['future'].values())\n",
    "    verbs.extend(profile['paradigm']['formal']['Persian']['negative']['present'].values())\n",
    "    verbs.extend(profile['paradigm']['formal']['Persian']['negative']['past'].values())\n",
    "    verbs.extend(profile['paradigm']['formal']['Persian']['negative']['future'].values())\n",
    "    \n",
    "    verbs = [list(gro.values()) for gro in verbs if gro]\n",
    "    \n",
    "    groups = [flatten_list(gro) for gro in verbs if gro != None]\n",
    "    \n",
    "    return groups\n",
    "\n",
    "groups = get_allshapes('هست')\n",
    "groups[random.randint(0 ,len(groups))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hazm import POSTagger, word_tokenize\n",
    "\n",
    "tagger = POSTagger(model='../resources/postagger.model')\n",
    "\n",
    "def get_with_allverbs(text):\n",
    "    \n",
    "    verbs = [tg for tg in tagger.tag(word_tokenize(text)) if tg[1] == 'V']\n",
    "    \n",
    "    replaced = []\n",
    "    \n",
    "    for vr, _ in verbs:\n",
    "        \n",
    "        for vrps in get_allshapes(vr):\n",
    "            if vr not in vrps:\n",
    "                continue\n",
    "                \n",
    "            replaced.extend([text.replace(vr, new) for new in vrps])\n",
    "            \n",
    "    return replaced\n",
    "\n",
    "for text in df.commentText.sample(1):\n",
    "    new_texts = ['%s: %s' % x for x in enumerate(get_with_allverbs(text), start=1)]\n",
    "    print('orig:', text, *new_texts, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scenario 6: Normalize Unconventional Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hazm import word_tokenize\n",
    "from collections import Counter\n",
    "\n",
    "counter = Counter()\n",
    "df.no_stopwords.apply(lambda x: counter.update(word_tokenize(x)))\n",
    "\n",
    "counter.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "half_commens = set(x[0]for x in counter.most_common(len(counter) // 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def normalize_badwords(word):\n",
    "    searched = re.search(r'(.)\\1{1,}', word)\n",
    "    if not searched:\n",
    "        return word\n",
    "    \n",
    "    one = word[:searched.span()[0] + 1] + word[searched.span()[1]:]\n",
    "    \n",
    "    if word[searched.span()[0] + 1] != 'ی':\n",
    "        return one\n",
    "    \n",
    "    two = word[:searched.span()[0] + 2] + word[searched.span()[1]:]\n",
    "    return one if one in half_commens else two\n",
    "\n",
    "print(normalize_badwords('عالیییییه'), ' و ', normalize_badwords('پاییییین'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hazm import word_tokenize\n",
    "\n",
    "def normalize_badtexts(text):\n",
    "    return ' '.join([normalize_badwords(to) for to in word_tokenize(text)])\n",
    "\n",
    "print(normalize_badtexts('سلاااااام به‌به!!!! چه تی‌ای گلییییییی. درخدمت باشییییم'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Final Check For Achieving The Right Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hazm import Normalizer, stopwords_list, word_tokenize, Lemmatizer, Stemmer\n",
    "\n",
    "normalizer = Normalizer()\n",
    "df['normalized'] = df.commentText.apply(lambda x: normalizer.normalize(x).strip())\n",
    "\n",
    "stemmer = Stemmer()\n",
    "lemmatizer = Lemmatizer()\n",
    "\n",
    "def context_exchanger(word):\n",
    "    \n",
    "    lemmatized, stemmed = lemmatizer.lemmatize(word), stemmer.stem(word) \n",
    "    \n",
    "    return (('ن' if word.startswith('ن') and not lemmatized.split('#')[1].startswith('ن') else '') \n",
    "            + lemmatized.split('#')[1]) if '#' in lemmatized else stemmed\n",
    "\n",
    "df['lemz_or_stem'] = df.normalized.apply(\n",
    "    lambda x: ' '.join((context_exchanger(y) for y in word_tokenize(x))))\n",
    "\n",
    "stopwords = set(stopwords_list() + [])\n",
    "df['no_stopwords'] = df.normalized.apply(\n",
    "    lambda x: ' '.join((y for y in word_tokenize(x) if y not in stopwords)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import option_context\n",
    "\n",
    "with option_context('display.max_colwidth', 400):\n",
    "    display(df[['no_stopwords', 'feeling']].sample(10))\n",
    "    \n",
    "with option_context('display.max_colwidth', 400):\n",
    "    display(df[['lemz_or_stem', 'feeling']].sample(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construction Of Final Dataset According To The Scenarios "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hazm import POSTagger, word_tokenize, Lemmatizer, Normalizer, stopwords_list\n",
    "from collections import Counter\n",
    "from langdetect import detect\n",
    "\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./comment-feeling-1650617369.csv')\n",
    "\n",
    "def is_persian(text):\n",
    "    try: return detect(text) == 'fa' \n",
    "    except: return False\n",
    "    \n",
    "df = df[df.commentText.apply(lambda x: is_persian(x))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(stopwords_list())\n",
    "df['no_stopwords'] = df.commentText.apply(\n",
    "    lambda x: ' '.join((y for y in word_tokenize(x) if y not in stopwords)))\n",
    "\n",
    "df = df[df.no_stopwords.str.count(' ') > 0]\n",
    "\n",
    "counter = Counter()\n",
    "df.no_stopwords.apply(lambda x: counter.update(word_tokenize(x)))\n",
    "half_commens = set(x[0]for x in counter.most_common(len(counter) // 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_badwords(word):\n",
    "    searched = re.search(r'(.)\\1{1,}', word)\n",
    "    if not searched:\n",
    "        return word\n",
    "    \n",
    "    one = word[:searched.span()[0] + 1] + word[searched.span()[1]:]\n",
    "    \n",
    "    if word[searched.span()[0] + 1] != 'ی':\n",
    "        return one\n",
    "    \n",
    "    two = word[:searched.span()[0] + 2] + word[searched.span()[1]:]\n",
    "    return one if one in half_commens else two\n",
    "\n",
    "\n",
    "tagger = POSTagger(model='../resources/postagger.model')\n",
    "lemmatizer = Lemmatizer()\n",
    "normalizer = Normalizer()\n",
    "\n",
    "\n",
    "lemmatize = lambda word, _type: lemmatizer.lemmatize(word) if _type != 'V' else word\n",
    "add_not = lambda old, new: old.startswith('ن') and not new.startswith('ن')\n",
    "change = lambda word, _type: _type == 'V' and '#' in word and add_not(*word.split('#'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setp_1(text):\n",
    "    \n",
    "    tokens = [token.replace('#', '_') for token in word_tokenize(text)]\n",
    "    tokens = [normalize_badwords(token) for token in tokens]\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "\n",
    "def setp_3(tags):\n",
    "    return [tag for tag in tags if tag[1] not in {'P', 'CONJ', 'PRO', 'DET'}]\n",
    "\n",
    "\n",
    "df['step_1'] = df.commentText.apply(setp_1)\n",
    "df['step_2'] = tagger.tag_sents(df.step_1.apply(lambda x: x.split()))\n",
    "df['step_3'] = df.step_2.apply(setp_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proccessing(tags):\n",
    "    \n",
    "    lemmatizeds = [(lemmatize(*tag), tag[1]) for tag in tags]\n",
    "    prefixes = ['ن' if change(*tag) else '' for tag in lemmatizeds]\n",
    "    \n",
    "    tuples = zip(prefixes, lemmatizeds)\n",
    "    \n",
    "    lemmatizeds = [(pre + word, _type) for pre, (word, _type) in tuples]\n",
    "    return normalizer.normalize(' '.join([x for x, y in lemmatizeds])).strip()\n",
    "\n",
    "\n",
    "df['preprocessed'] = df.step_3.apply(proccessing)\n",
    "df['normalized'] = df.commentText.apply(normalizer.normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.concat(\n",
    "    (df[['normalized'  , 'feeling']].rename(columns={\"normalized\"  : \"text\", \"feeling\": \"feeling\"}),\n",
    "     df[['preprocessed', 'feeling']].rename(columns={\"preprocessed\": \"text\", \"feeling\": \"feeling\"}), \n",
    "    ))\n",
    "\n",
    "import time\n",
    "dataset.to_csv(f'Pre-processed-comments-{int(time.time())}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ir-venv",
   "language": "python",
   "name": "ir-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
