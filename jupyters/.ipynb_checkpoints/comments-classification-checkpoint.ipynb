{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boilerplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install numpy pandas matplotlib torch sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to accomplish attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_matmul_bias(seq, weight, bias, nonlinearity=''):\n",
    "    s = None\n",
    "    bias_dim = bias.size()\n",
    "    for i in range(seq.size(0)):\n",
    "        _s = torch.mm(seq[i], weight) \n",
    "        _s_bias = _s + bias.expand(bias_dim[0], _s.size()[0]).transpose(0,1)\n",
    "        if(nonlinearity=='tanh'):\n",
    "            _s_bias = torch.tanh(_s_bias)\n",
    "        _s_bias = _s_bias.unsqueeze(0)\n",
    "        if(s is None):\n",
    "            s = _s_bias\n",
    "        else:\n",
    "            s = torch.cat((s,_s_bias),0)\n",
    "            \n",
    "    return s.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_matmul(seq, weight, nonlinearity=''):\n",
    "    s = None\n",
    "    for i in range(seq.size(0)):\n",
    "        _s = torch.mm(seq[i], weight)\n",
    "        if(nonlinearity=='tanh'):\n",
    "            _s = torch.tanh(_s)\n",
    "        _s = _s.unsqueeze(0)\n",
    "        if(s is None):\n",
    "            s = _s\n",
    "        else:\n",
    "            s = torch.cat((s,_s), 0)\n",
    "            \n",
    "    return s.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_mul(rnn_outputs, att_weights):\n",
    "    attn_vectors = None\n",
    "    for i in range(rnn_outputs.size(0)):\n",
    "        h_i = rnn_outputs[i]\n",
    "        a_i = att_weights[i].unsqueeze(1).expand_as(h_i)\n",
    "        h_i = a_i * h_i\n",
    "        h_i = h_i.unsqueeze(0)\n",
    "        if(attn_vectors is None):\n",
    "            attn_vectors = h_i\n",
    "        else:\n",
    "            attn_vectors = torch.cat((attn_vectors,h_i),0)\n",
    "    return torch.sum(attn_vectors, 0).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word attention model with bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionWordRNN(nn.Module):\n",
    "    \n",
    "    \n",
    "    def __init__(self, batch_size, num_tokens, embed_size, word_gru_hidden, bidirectional= True):        \n",
    "        \n",
    "        super(AttentionWordRNN, self).__init__()\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.num_tokens = num_tokens\n",
    "        self.embed_size = embed_size\n",
    "        self.word_gru_hidden = word_gru_hidden\n",
    "        self.bidirectional = bidirectional\n",
    "        \n",
    "        self.lookup = nn.Embedding(num_tokens, embed_size)\n",
    "        if bidirectional == True:\n",
    "            self.word_gru = nn.GRU(embed_size, word_gru_hidden, bidirectional= True)\n",
    "            self.weight_W_word = nn.Parameter(torch.Tensor(2* word_gru_hidden,2*word_gru_hidden))\n",
    "            self.bias_word = nn.Parameter(torch.Tensor(2* word_gru_hidden,1))\n",
    "            self.weight_proj_word = nn.Parameter(torch.Tensor(2*word_gru_hidden, 1))\n",
    "        else:\n",
    "            self.word_gru = nn.GRU(embed_size, word_gru_hidden, bidirectional= False)\n",
    "            self.weight_W_word = nn.Parameter(torch.Tensor(word_gru_hidden, word_gru_hidden))\n",
    "            self.bias_word = nn.Parameter(torch.Tensor(word_gru_hidden,1))\n",
    "            self.weight_proj_word = nn.Parameter(torch.Tensor(word_gru_hidden, 1))\n",
    "            \n",
    "        self.softmax_word = nn.Softmax()\n",
    "        self.weight_W_word.data.uniform_(-0.1, 0.1)\n",
    "        self.weight_proj_word.data.uniform_(-0.1,0.1)\n",
    "\n",
    "        \n",
    "        \n",
    "    def forward(self, embed, state_word):\n",
    "        # embeddings\n",
    "        embedded = self.lookup(embed)\n",
    "        # word level gru\n",
    "        output_word, state_word = self.word_gru(embedded, state_word)\n",
    "        word_squish = batch_matmul_bias(output_word, self.weight_W_word, self.bias_word, nonlinearity='tanh')\n",
    "        word_squish = word_squish.reshape(\n",
    "            output_word.shape[0], output_word.shape[1], self.weight_W_word.shape[1])\n",
    "\n",
    "        word_attn = batch_matmul(word_squish, self.weight_proj_word)\n",
    "        word_attn = word_attn.reshape(word_squish.shape[0], word_squish.shape[1])\n",
    "        word_attn_norm = self.softmax_word(word_attn.transpose(1, 0))\n",
    "        word_attn_vectors = attention_mul(output_word, word_attn_norm.transpose(1, 0))\n",
    "        return word_attn_vectors, state_word, word_attn_norm\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        if self.bidirectional == True:\n",
    "            return Variable(torch.zeros(2, self.batch_size, self.word_gru_hidden))\n",
    "        else:\n",
    "            return Variable(torch.zeros(1, self.batch_size, self.word_gru_hidden))        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Attention model with bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionSentRNN(nn.Module):\n",
    "    \n",
    "    \n",
    "    def __init__(self, batch_size, sent_gru_hidden, word_gru_hidden, n_classes, bidirectional= True):        \n",
    "        \n",
    "        super(AttentionSentRNN, self).__init__()\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.sent_gru_hidden = sent_gru_hidden\n",
    "        self.n_classes = n_classes\n",
    "        self.word_gru_hidden = word_gru_hidden\n",
    "        self.bidirectional = bidirectional\n",
    "        \n",
    "        \n",
    "        if bidirectional == True:\n",
    "            self.sent_gru = nn.GRU(2 * word_gru_hidden, sent_gru_hidden, bidirectional= True)        \n",
    "            self.weight_W_sent = nn.Parameter(torch.Tensor(2* sent_gru_hidden ,2* sent_gru_hidden))\n",
    "            self.bias_sent = nn.Parameter(torch.Tensor(2* sent_gru_hidden,1))\n",
    "            self.weight_proj_sent = nn.Parameter(torch.Tensor(2* sent_gru_hidden, 1))\n",
    "            self.final_linear = nn.Linear(2* sent_gru_hidden, n_classes)\n",
    "        else:\n",
    "            self.sent_gru = nn.GRU(word_gru_hidden, sent_gru_hidden, bidirectional= True)        \n",
    "            self.weight_W_sent = nn.Parameter(torch.Tensor(sent_gru_hidden ,sent_gru_hidden))\n",
    "            self.bias_sent = nn.Parameter(torch.Tensor(sent_gru_hidden,1))\n",
    "            self.weight_proj_sent = nn.Parameter(torch.Tensor(sent_gru_hidden, 1))\n",
    "            self.final_linear = nn.Linear(sent_gru_hidden, n_classes)\n",
    "        self.softmax_sent = nn.Softmax()\n",
    "        self.final_softmax = nn.Softmax()\n",
    "        self.weight_W_sent.data.uniform_(-0.1, 0.1)\n",
    "        self.weight_proj_sent.data.uniform_(-0.1,0.1)\n",
    "\n",
    "        \n",
    "        \n",
    "    def forward(self, word_attention_vectors, state_sent):\n",
    "        output_sent, state_sent = self.sent_gru(word_attention_vectors, state_sent)\n",
    "        sent_squish = batch_matmul_bias(output_sent, self.weight_W_sent, self.bias_sent, nonlinearity='tanh')\n",
    "        sent_squish = sent_squish.reshape(\n",
    "            output_sent.shape[0], output_sent.shape[1], self.weight_W_sent.shape[1])\n",
    "        \n",
    "        sent_attn = batch_matmul(sent_squish, self.weight_proj_sent)\n",
    "        sent_attn = sent_attn.reshape(sent_squish.shape[0], sent_squish.shape[1])\n",
    "        sent_attn_norm = self.softmax_sent(sent_attn.transpose(1, 0))\n",
    "        sent_attn_vectors = attention_mul(output_sent, sent_attn_norm.transpose(1, 0))\n",
    "        final_map = self.final_linear(sent_attn_vectors.squeeze(0))\n",
    "        return F.log_softmax(final_map), state_sent, sent_attn_norm\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        if self.bidirectional == True:\n",
    "            return Variable(torch.zeros(2, self.batch_size, self.sent_gru_hidden))\n",
    "        else:\n",
    "            return Variable(torch.zeros(1, self.batch_size, self.sent_gru_hidden))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Pre-processed-comments-1650746640.csv')\n",
    "\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from hazm import word_tokenize, sent_tokenize\n",
    "\n",
    "vocab = build_vocab_from_iterator(df.text.apply(word_tokenize), specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_attn = AttentionWordRNN(batch_size=64, num_tokens=len(vocab), embed_size=300, \n",
    "                             word_gru_hidden=100, bidirectional= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_attn = AttentionSentRNN(batch_size=64, sent_gru_hidden=100, word_gru_hidden=100, \n",
    "                             n_classes=2, bidirectional= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_data(mini_batch, targets, word_attn_model, sent_attn_model, word_optimizer, sent_optimizer, criterion):\n",
    "    state_word = word_attn_model.init_hidden().to(device)\n",
    "    state_sent = sent_attn_model.init_hidden().to(device)\n",
    "    max_sents, batch_size, max_tokens = mini_batch.size()\n",
    "    word_optimizer.zero_grad()\n",
    "    sent_optimizer.zero_grad()\n",
    "    s = None\n",
    "    for i in range(max_sents):\n",
    "        _s, state_word, _ = word_attn_model(mini_batch[i,:,:].transpose(0,1), state_word)\n",
    "        if(s is None):\n",
    "            s = _s\n",
    "        else:\n",
    "            s = torch.cat((s,_s),0)            \n",
    "    y_pred, state_sent, _ = sent_attn_model(s, state_sent)\n",
    "    loss = criterion(y_pred.to(device), targets) \n",
    "    loss.backward()\n",
    "    \n",
    "    word_optimizer.step()\n",
    "    sent_optimizer.step()\n",
    "    \n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(val_tokens, word_attn_model, sent_attn_model):\n",
    "    max_sents, batch_size, max_tokens = val_tokens.size()\n",
    "    state_word = word_attn_model.init_hidden().to(device)\n",
    "    state_sent = sent_attn_model.init_hidden().to(device)\n",
    "    s = None\n",
    "    for i in range(max_sents):\n",
    "        _s, state_word, _ = word_attn_model(val_tokens[i,:,:].transpose(0,1), state_word)\n",
    "        if(s is None):\n",
    "            s = _s\n",
    "        else:\n",
    "            s = torch.cat((s,_s),0)            \n",
    "    y_pred, state_sent, _ = sent_attn_model(s, state_sent)    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-1\n",
    "momentum = 0.9\n",
    "\n",
    "word_optmizer = torch.optim.SGD(word_attn.parameters(), lr=learning_rate, momentum= momentum)\n",
    "sent_optimizer = torch.optim.SGD(sent_attn.parameters(), lr=learning_rate, momentum= momentum)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(AttentionWordRNN(\n",
       "   (lookup): Embedding(26029, 300)\n",
       "   (word_gru): GRU(300, 100, bidirectional=True)\n",
       "   (softmax_word): Softmax(dim=None)\n",
       " ),\n",
       " AttentionSentRNN(\n",
       "   (sent_gru): GRU(200, 100, bidirectional=True)\n",
       "   (final_linear): Linear(in_features=200, out_features=2, bias=True)\n",
       "   (softmax_sent): Softmax(dim=None)\n",
       "   (final_softmax): Softmax(dim=None)\n",
       " ))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_attn.to(device), sent_attn.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(87530,)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Pre-processed-comments-1650746640.csv')\n",
    "\n",
    "from hazm import word_tokenize, sent_tokenize\n",
    "\n",
    "def tokenize(text):\n",
    "    try: return [vocab(word_tokenize(s)) for s in sent_tokenize(text)]\n",
    "    except: return\n",
    "\n",
    "df['tokens'] = df.text.apply(lambda x: tokenize(x))\n",
    "\n",
    "df = df[~df.tokens.isna()]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df.tokens.values, df.feeling.values, test_size = 0.3, random_state= 42)\n",
    "\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_batch(mini_batch):\n",
    "    \n",
    "    mini_batch_size = len(mini_batch)\n",
    "    max_sent_len = int(np.max([len(x) for x in mini_batch]))\n",
    "    max_token_len = int(np.max([len(val) for sublist in mini_batch for val in sublist]))\n",
    "    main_matrix = np.zeros((mini_batch_size, max_sent_len, max_token_len), dtype= np.int)\n",
    "    \n",
    "    for i in range(main_matrix.shape[0]):\n",
    "        for j in range(main_matrix.shape[1]):\n",
    "            for k in range(main_matrix.shape[2]):\n",
    "                try:\n",
    "                    main_matrix[i,j,k] = mini_batch[i][j][k]\n",
    "                except IndexError:\n",
    "                    pass\n",
    "                \n",
    "    return Variable(torch.from_numpy(main_matrix).transpose(0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_accuracy_mini_batch(tokens, labels, word_attn, sent_attn):\n",
    "    y_pred = get_predictions(tokens, word_attn, sent_attn)\n",
    "    _, y_pred = torch.max(y_pred, 1)\n",
    "    correct = np.ndarray.flatten(y_pred.data.cpu().numpy())\n",
    "    labels = np.ndarray.flatten(labels.data.cpu().numpy())\n",
    "    num_correct = sum(correct == labels)\n",
    "    return float(num_correct) / len(correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_accuracy_full_batch(tokens, labels, mini_batch_size, word_attn, sent_attn):\n",
    "    p = []\n",
    "    l = []\n",
    "    g = gen_minibatch(tokens, labels, mini_batch_size)\n",
    "    for token, label in g:\n",
    "        y_pred = get_predictions(token.to(device), word_attn, sent_attn)\n",
    "        _, y_pred = torch.max(y_pred, 1)\n",
    "        p.append(np.ndarray.flatten(y_pred.data.cpu().numpy()))\n",
    "        l.append(np.ndarray.flatten(label.data.cpu().numpy()))\n",
    "    p = [item for sublist in p for item in sublist]\n",
    "    l = [item for sublist in l for item in sublist]\n",
    "    p = np.array(p)\n",
    "    l = np.array(l)\n",
    "    num_correct = sum(p == l)\n",
    "    return float(num_correct)/ len(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_data(mini_batch, targets, word_attn_model, sent_attn_model):    \n",
    "    state_word = word_attn_model.init_hidden().to(device)\n",
    "    state_sent = sent_attn_model.init_hidden().to(device)\n",
    "    max_sents, batch_size, max_tokens = mini_batch.size()\n",
    "    s = None\n",
    "    for i in range(max_sents):\n",
    "        _s, state_word, _ = word_attn_model(mini_batch[i,:,:].transpose(0,1), state_word)\n",
    "        if(s is None):\n",
    "            s = _s\n",
    "        else:\n",
    "            s = torch.cat((s,_s),0)            \n",
    "    y_pred, state_sent,_ = sent_attn_model(s, state_sent)\n",
    "    loss = criterion(y_pred.to(device), targets)     \n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n",
    "    assert inputs.shape[0] == targets.shape[0]\n",
    "    if shuffle:\n",
    "        indices = np.arange(inputs.shape[0])\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, inputs.shape[0] - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield inputs[excerpt], targets[excerpt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_minibatch(tokens, labels, mini_batch_size, shuffle= True):\n",
    "    for token, label in iterate_minibatches(tokens, labels, mini_batch_size, shuffle= shuffle):\n",
    "        token = pad_batch(token)\n",
    "        yield token.to(device), Variable(torch.from_numpy(label), requires_grad= False).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_val_loss(val_tokens, val_labels, mini_batch_size, word_attn_model, sent_attn_model):\n",
    "    val_loss = []\n",
    "    for token, label in iterate_minibatches(val_tokens, val_labels, mini_batch_size, shuffle= True):\n",
    "        val_loss.append(\n",
    "            test_data(\n",
    "                pad_batch(token).to(device), \n",
    "                Variable(torch.from_numpy(label), requires_grad= False).to(device), \n",
    "                word_attn_model, \n",
    "                sent_attn_model)\n",
    "        )\n",
    "    return np.mean(val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_early_stopping(mini_batch_size, X_train, y_train, X_test, y_test, word_attn_model, sent_attn_model, \n",
    "                         word_attn_optimiser, sent_attn_optimiser, loss_criterion, iterations, \n",
    "                         print_val_loss_every = 1000, print_loss_every = 50):\n",
    "    start = time.time()\n",
    "    \n",
    "    loss_full = []\n",
    "    loss_epoch = []\n",
    "    loss_smooth = []\n",
    "    accuracy_full = []\n",
    "    accuracy_epoch = []\n",
    "    epoch_counter = 0\n",
    "    \n",
    "    g = gen_minibatch(X_train, y_train, mini_batch_size)\n",
    "    \n",
    "    for i in range(1, iterations + 1):\n",
    "        try:\n",
    "            tokens, labels = next(g)\n",
    "            loss = train_data(\n",
    "                tokens, labels, word_attn_model, sent_attn_model, \n",
    "                word_attn_optimiser, sent_attn_optimiser, loss_criterion)\n",
    "            \n",
    "            loss_full.append(loss)\n",
    "            loss_epoch.append(loss)\n",
    "            \n",
    "            acc = test_accuracy_mini_batch(tokens, labels, word_attn_model, sent_attn_model)\n",
    "            \n",
    "            accuracy_full.append(acc)\n",
    "            accuracy_epoch.append(acc)\n",
    "            \n",
    "            # print loss every n passes\n",
    "            if i % print_loss_every == 0:\n",
    "                print('Loss at %d minibatches, %d epoch,(%s) is %f' %(i, epoch_counter, timeSince(start), np.mean(loss_epoch)))\n",
    "                print('Accuracy at %d minibatches is %f' % (i, np.mean(accuracy_epoch)))\n",
    "                \n",
    "            # check validation loss every n passes\n",
    "            if i % print_val_loss_every == 0:\n",
    "                val_loss = check_val_loss(X_test, y_test, mini_batch_size, word_attn_model, sent_attn_model)\n",
    "                print('Average training loss at this epoch..minibatch..%d..is %f' % (i, np.mean(loss_epoch)))\n",
    "                print('Validation loss after %d passes is %f' %(i, val_loss))\n",
    "                \n",
    "                if val_loss > np.mean(loss_full):\n",
    "                    print('Validation loss is higher than training loss at %d is %f , stopping training!' % (i, val_loss))\n",
    "                    print('Average training loss at %d is %f' % (i, np.mean(loss_full)))\n",
    "                    \n",
    "        except StopIteration:\n",
    "            epoch_counter += 1\n",
    "            print('Reached %d epocs' % epoch_counter)\n",
    "            print('i %d' % i)\n",
    "            \n",
    "            g = gen_minibatch(X_train, y_train, mini_batch_size)\n",
    "            loss_epoch, accuracy_epoch = [], []\n",
    "\n",
    "    return loss_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at 100 minibatches, 0 epoch,(1m 53s) is 0.878781\n",
      "Accuracy at 100 minibatches is 0.817813\n",
      "Loss at 200 minibatches, 0 epoch,(4m 24s) is 0.779223\n",
      "Accuracy at 200 minibatches is 0.814609\n",
      "Loss at 300 minibatches, 0 epoch,(6m 32s) is 0.753488\n",
      "Accuracy at 300 minibatches is 0.816042\n",
      "Loss at 400 minibatches, 0 epoch,(8m 58s) is 0.706916\n",
      "Accuracy at 400 minibatches is 0.822695\n",
      "Loss at 500 minibatches, 0 epoch,(11m 6s) is 0.654303\n",
      "Accuracy at 500 minibatches is 0.830187\n",
      "Loss at 600 minibatches, 0 epoch,(13m 29s) is 0.634232\n",
      "Accuracy at 600 minibatches is 0.832031\n",
      "Loss at 700 minibatches, 0 epoch,(15m 54s) is 0.636330\n",
      "Accuracy at 700 minibatches is 0.828884\n",
      "Loss at 800 minibatches, 0 epoch,(18m 20s) is 0.628280\n",
      "Accuracy at 800 minibatches is 0.830020\n",
      "Loss at 900 minibatches, 0 epoch,(20m 46s) is 0.626704\n",
      "Accuracy at 900 minibatches is 0.828941\n"
     ]
    }
   ],
   "source": [
    "# from google.colab import files\n",
    "\n",
    "loss_full = train_early_stopping(\n",
    "    64, X_train, y_train, X_test, y_test, \n",
    "    word_attn, sent_attn, word_optmizer, sent_optimizer, criterion, 2700, 1000, 100)\n",
    "\n",
    "import time\n",
    "import torch\n",
    "\n",
    "timestamp = int(time.time())\n",
    "\n",
    "torch.save(word_attn.state_dict(), f'./word_attn-{timestamp}')\n",
    "torch.save(sent_attn.state_dict(), f'./sent_attn-{timestamp}')\n",
    "\n",
    "# files.download(f'./word_attn-{timestamp}')\n",
    "# files.download(f'./sent_attn-{timestamp}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy_full_batch(X_test, y_test, 64, word_attn, sent_attn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy_full_batch(X_train, y_train, 64, word_attn, sent_attn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text, word_attn_model, sent_attn_model):\n",
    "    \n",
    "    tokens = [tokenize(text)]\n",
    "    val_tokens = pad_batch(tokens)\n",
    "    max_sents, batch_size, max_tokens = val_tokens.size()\n",
    "    \n",
    "    state_word = Variable(torch.zeros(2, 1, 100)).to(device)\n",
    "    state_sent = Variable(torch.zeros(2, 1, 100)).to(device)\n",
    "    \n",
    "    s = None\n",
    "    for i in range(max_sents):\n",
    "        \n",
    "        _s, state_word, _ = word_attn_model(\n",
    "            val_tokens[i,:,:].transpose(0,1), state_word)\n",
    "        \n",
    "        if(s is None): s = _s\n",
    "        else: s = torch.cat((s,_s),0) \n",
    "            \n",
    "    y_pred, state_sent, _ = sent_attn_model(s, state_sent)  \n",
    "    _, y_pred = torch.max(y_pred, 1)\n",
    "    \n",
    "    return {0.:'SAD', 1.:'HAPPY'}[y_pred.item()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict('واقعا غذای بدی بود', word_attn, sent_attn) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "ir-venv",
   "language": "python",
   "name": "ir-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
